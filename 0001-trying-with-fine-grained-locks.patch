From b2f1540e07a39dd85695b7158c6fab9660707ea8 Mon Sep 17 00:00:00 2001
From: Aaron <kljsandjb@me.com>
Date: Mon, 20 Jul 2020 14:09:37 +0000
Subject: [PATCH] trying with fine-grained locks...

1. page free list lock
2. scheduler lock (similar to env list lock)
---
 kern/env.c      |  7 +++++--
 kern/init.c     | 12 ++----------
 kern/pmap.c     |  6 ++++++
 kern/sched.c    |  3 ++-
 kern/spinlock.c | 14 ++++++++++++++
 kern/spinlock.h | 28 ++++++++++++++++++++++++++++
 kern/syscall.c  |  3 +++
 kern/trap.c     | 15 +++++++++------
 8 files changed, 69 insertions(+), 19 deletions(-)

diff --git a/kern/env.c b/kern/env.c
index e1019da..23a971e 100644
--- a/kern/env.c
+++ b/kern/env.c
@@ -236,7 +236,7 @@ env_alloc(struct Env **newenv_store, envid_t parent_id)
 	// Set the basic status variables.
 	e->env_parent_id = parent_id;
 	e->env_type = ENV_TYPE_USER;
-	e->env_status = ENV_RUNNABLE;
+	e->env_status = ENV_NOT_RUNNABLE;
 	e->env_runs = 0;
 
 	// Clear out all the saved register state,
@@ -417,6 +417,7 @@ env_create(uint8_t *binary, enum EnvType type)
 		panic("env_alloc");
 	load_icode(new_env, binary);
 	new_env->env_type = type;
+	new_env->env_status = ENV_RUNNABLE;
 }
 
 //
@@ -492,6 +493,7 @@ env_destroy(struct Env *e)
 
 	if (curenv == e) {
 		curenv = NULL;
+		unlock_kernel();
 		sched_yield();
 	}
 }
@@ -557,7 +559,8 @@ env_run(struct Env *e)
 	curenv->env_runs ++;
 
 	lcr3(PADDR(curenv->env_pgdir));
-	unlock_kernel();
+	if (scheduler_lock.locked && scheduler_lock.cpu == thiscpu)
+		unlock_scheduler();
 	env_pop_tf(&(curenv->env_tf));
 }
 
diff --git a/kern/init.c b/kern/init.c
index 0a0faaa..62c4af7 100644
--- a/kern/init.c
+++ b/kern/init.c
@@ -13,7 +13,6 @@
 #include <kern/sched.h>
 #include <kern/picirq.h>
 #include <kern/cpu.h>
-#include <kern/spinlock.h>
 
 static void boot_aps(void);
 
@@ -41,13 +40,6 @@ i386_init(void)
 	// Lab 4 multitasking initialization functions
 	pic_init();
 
-	// Acquire the big kernel lock before waking up APs
-	// Your code here:
-	lock_kernel();
-
-	// Starting non-boot CPUs
-	boot_aps();
-
 #if defined(TEST)
 	// Don't touch -- used by grading script!
 	ENV_CREATE(TEST, ENV_TYPE_USER);
@@ -62,7 +54,8 @@ i386_init(void)
 	ENV_CREATE(user_yield, ENV_TYPE_USER);
 	ENV_CREATE(user_yield, ENV_TYPE_USER);
 #endif // TEST*
-
+    // Starting non-boot CPUs
+	boot_aps();
 	// Schedule and run the first user environment!
 	sched_yield();
 }
@@ -117,7 +110,6 @@ mp_main(void)
 	// only one CPU can enter the scheduler at a time!
 	//
 	// Your code here:
-	lock_kernel();
 	sched_yield();
 
 	// Remove this after you finish Exercise 6
diff --git a/kern/pmap.c b/kern/pmap.c
index c2e22a7..a598f78 100644
--- a/kern/pmap.c
+++ b/kern/pmap.c
@@ -10,6 +10,7 @@
 #include <kern/kclock.h>
 #include <kern/env.h>
 #include <kern/cpu.h>
+#include <kern/spinlock.h>
 
 // These variables are set by i386_detect_memory()
 size_t npages;			// Amount of physical memory (in pages)
@@ -387,14 +388,17 @@ struct PageInfo *
 page_alloc(int alloc_flags)
 {
 	// Fill this function in
+	lock_pagelist();
 	if (page_free_list == NULL)
 	{
+		unlock_pagelist();
 		return NULL;
 	}
 
 	struct PageInfo *alloc_page;
 	alloc_page = page_free_list;
 	page_free_list = page_free_list->pp_link;
+	unlock_pagelist();
 	alloc_page->pp_link = NULL;
 
 	if (alloc_flags & ALLOC_ZERO)
@@ -423,8 +427,10 @@ page_free(struct PageInfo *pp)
 	{
 		panic("Page is still in use! Cannot be freed");
 	}
+	lock_pagelist();
 	pp->pp_link = page_free_list;
 	page_free_list = pp;
+	unlock_pagelist();
 }
 
 //
diff --git a/kern/sched.c b/kern/sched.c
index 9e464d7..a9944c5 100644
--- a/kern/sched.c
+++ b/kern/sched.c
@@ -33,6 +33,7 @@ sched_yield(void)
 	int i = 0, curpos = 0, k = 0;
 	if (curenv)
 		curpos = ENVX(curenv->env_id);
+	lock_scheduler();
 	for (; i < NENV; i++)
 	{
 		k = (i + curpos) % NENV;		// in a circular way
@@ -82,7 +83,7 @@ sched_halt(void)
 	xchg(&thiscpu->cpu_status, CPU_HALTED);
 
 	// Release the big kernel lock as if we were "leaving" the kernel
-	unlock_kernel();
+	unlock_scheduler();
 
 	// Reset stack pointer, enable interrupts and then halt.
 	asm volatile (
diff --git a/kern/spinlock.c b/kern/spinlock.c
index bf4d2d2..be4c253 100644
--- a/kern/spinlock.c
+++ b/kern/spinlock.c
@@ -16,6 +16,20 @@ struct spinlock kernel_lock = {
 #endif
 };
 
+// the page free list lock
+struct spinlock pagelist_lock = {
+#ifdef DEBUG_SPINLOCK
+	.name = "pagelist_lock"
+#endif
+};
+
+// scheduler lock
+struct spinlock scheduler_lock = {
+#ifdef DEBUG_SPINLOCK
+	.name = "scheduler_lock"
+#endif
+};
+
 #ifdef DEBUG_SPINLOCK
 // Record the current call stack in pcs[] by following the %ebp chain.
 static void
diff --git a/kern/spinlock.h b/kern/spinlock.h
index 52d20b4..51256dc 100644
--- a/kern/spinlock.h
+++ b/kern/spinlock.h
@@ -26,6 +26,34 @@ void spin_unlock(struct spinlock *lk);
 #define spin_initlock(lock)   __spin_initlock(lock, #lock)
 
 extern struct spinlock kernel_lock;
+extern struct spinlock pagelist_lock;
+extern struct spinlock scheduler_lock;
+
+static inline void
+lock_pagelist(void)
+{
+	spin_lock(&pagelist_lock);
+}
+
+static inline void
+unlock_pagelist(void)
+{
+	spin_unlock(&pagelist_lock);
+	asm volatile("pause");
+}
+
+static inline void
+lock_scheduler(void)
+{
+	spin_lock(&scheduler_lock);
+}
+
+static inline void
+unlock_scheduler(void)
+{
+	spin_unlock(&scheduler_lock);
+	asm volatile("pause");
+}
 
 static inline void
 lock_kernel(void)
diff --git a/kern/syscall.c b/kern/syscall.c
index f19a51f..230ecf2 100644
--- a/kern/syscall.c
+++ b/kern/syscall.c
@@ -11,6 +11,7 @@
 #include <kern/syscall.h>
 #include <kern/console.h>
 #include <kern/sched.h>
+#include <kern/spinlock.h>
 
 // Print a string to the system console.
 // The string is exactly 'len' characters long.
@@ -68,6 +69,7 @@ sys_env_destroy(envid_t envid)
 static void
 sys_yield(void)
 {
+	unlock_kernel();
 	sched_yield();
 }
 
@@ -352,6 +354,7 @@ sys_ipc_recv(void *dstva)
 
 	// now we give up CPU
 	curenv->env_status = ENV_NOT_RUNNABLE;
+	unlock_kernel();
 	sched_yield();
 
 	return 0;
diff --git a/kern/trap.c b/kern/trap.c
index 59027a8..a981a75 100644
--- a/kern/trap.c
+++ b/kern/trap.c
@@ -243,6 +243,7 @@ trap_dispatch(struct Trapframe *tf)
 		switch (tf->tf_trapno)
 		{
 		case IRQ_OFFSET + IRQ_TIMER:
+			unlock_kernel();
 			sched_yield();
 			break;
 
@@ -275,19 +276,18 @@ trap(struct Trapframe *tf)
 
 	// Re-acqurie the big kernel lock if we were halted in
 	// sched_yield()
-	if (xchg(&thiscpu->cpu_status, CPU_STARTED) == CPU_HALTED)
-		lock_kernel();
+	xchg(&thiscpu->cpu_status, CPU_STARTED);
+		// lock_scheduler();
 	// Check that interrupts are disabled.  If this assertion
 	// fails, DO NOT be tempted to fix it by inserting a "cli" in
 	// the interrupt path.
 	assert(!(read_eflags() & FL_IF));
+	// Acquire the big kernel lock before doing any
+	// serious kernel work. only apply in trap
+	lock_kernel();
 
 	if ((tf->tf_cs & 3) == 3) {
 		// Trapped from user mode.
-		// Acquire the big kernel lock before doing any
-		// serious kernel work.
-		// LAB 4: Your code here.
-		lock_kernel();
 
 		// Question 2: Why do we still need separate kernel stacks for each CPU?
 		// Answer: 
@@ -302,6 +302,7 @@ trap(struct Trapframe *tf)
 		if (curenv->env_status == ENV_DYING) {
 			env_free(curenv);
 			curenv = NULL;
+			unlock_kernel();
 			sched_yield();
 		}
 
@@ -323,6 +324,7 @@ trap(struct Trapframe *tf)
 	// If we made it to this point, then no other environment was
 	// scheduled, so we should return to the current environment
 	// if doing so makes sense.
+	unlock_kernel();
 	if (curenv && curenv->env_status == ENV_RUNNING)
 		env_run(curenv);
 	else
@@ -408,6 +410,7 @@ page_fault_handler(struct Trapframe *tf)
 
 	tf->tf_esp = (uintptr_t)&utf->utf_fault_va;
 	tf->tf_eip = (uintptr_t)curenv->env_pgfault_upcall;
+	unlock_kernel();
 	env_run(curenv);
 end:
 	// Destroy the environment that caused the fault.
-- 
2.11.0

