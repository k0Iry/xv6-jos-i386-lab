From 4d3db7bc50a154377fd56592c4eb5587bacf307a Mon Sep 17 00:00:00 2001
From: Aaron <kljsandjb@me.com>
Date: Mon, 20 Jul 2020 14:09:37 +0000
Subject: [PATCH] trying with fine-grained locks...

1. page free list lock
2. scheduler lock (similar to env list lock)

added some comments
---
 kern/env.c      |  9 +++++++--
 kern/init.c     | 14 ++++----------
 kern/lapic.c    |  8 ++++++--
 kern/pmap.c     |  6 ++++++
 kern/sched.c    |  3 ++-
 kern/spinlock.c | 21 +++++++++++++++++++++
 kern/spinlock.h | 42 ++++++++++++++++++++++++++++++++++++++++++
 kern/syscall.c  |  3 +++
 kern/trap.c     | 15 +++++++++------
 9 files changed, 100 insertions(+), 21 deletions(-)

diff --git a/kern/env.c b/kern/env.c
index e1019da..648987e 100644
--- a/kern/env.c
+++ b/kern/env.c
@@ -90,6 +90,7 @@ envid2env(envid_t envid, struct Env **env_store, bool checkperm)
 	// that used the same slot in the envs[] array).
 	e = &envs[ENVX(envid)];
 	if (e->env_status == ENV_FREE || e->env_id != envid) {
+		cprintf("e->env_id 0x%x, envid: 0x%x\n", e->env_id, envid);
 		*env_store = 0;
 		return -E_BAD_ENV;
 	}
@@ -100,6 +101,7 @@ envid2env(envid_t envid, struct Env **env_store, bool checkperm)
 	// must be either the current environment
 	// or an immediate child of the current environment.
 	if (checkperm && e != curenv && e->env_parent_id != curenv->env_id) {
+		cprintf("e->env_parent_id 0x%x, curenv->env_id: 0x%x\n", e->env_parent_id, curenv->env_id);
 		*env_store = 0;
 		return -E_BAD_ENV;
 	}
@@ -236,7 +238,7 @@ env_alloc(struct Env **newenv_store, envid_t parent_id)
 	// Set the basic status variables.
 	e->env_parent_id = parent_id;
 	e->env_type = ENV_TYPE_USER;
-	e->env_status = ENV_RUNNABLE;
+	e->env_status = ENV_NOT_RUNNABLE;
 	e->env_runs = 0;
 
 	// Clear out all the saved register state,
@@ -417,6 +419,7 @@ env_create(uint8_t *binary, enum EnvType type)
 		panic("env_alloc");
 	load_icode(new_env, binary);
 	new_env->env_type = type;
+	new_env->env_status = ENV_RUNNABLE;
 }
 
 //
@@ -492,6 +495,7 @@ env_destroy(struct Env *e)
 
 	if (curenv == e) {
 		curenv = NULL;
+		unlock_kernel();
 		sched_yield();
 	}
 }
@@ -557,7 +561,8 @@ env_run(struct Env *e)
 	curenv->env_runs ++;
 
 	lcr3(PADDR(curenv->env_pgdir));
-	unlock_kernel();
+	if (scheduler_lock.locked && scheduler_lock.cpu == thiscpu)
+		unlock_scheduler();
 	env_pop_tf(&(curenv->env_tf));
 }
 
diff --git a/kern/init.c b/kern/init.c
index 0a0faaa..508d31c 100644
--- a/kern/init.c
+++ b/kern/init.c
@@ -13,7 +13,6 @@
 #include <kern/sched.h>
 #include <kern/picirq.h>
 #include <kern/cpu.h>
-#include <kern/spinlock.h>
 
 static void boot_aps(void);
 
@@ -41,13 +40,6 @@ i386_init(void)
 	// Lab 4 multitasking initialization functions
 	pic_init();
 
-	// Acquire the big kernel lock before waking up APs
-	// Your code here:
-	lock_kernel();
-
-	// Starting non-boot CPUs
-	boot_aps();
-
 #if defined(TEST)
 	// Don't touch -- used by grading script!
 	ENV_CREATE(TEST, ENV_TYPE_USER);
@@ -62,7 +54,8 @@ i386_init(void)
 	ENV_CREATE(user_yield, ENV_TYPE_USER);
 	ENV_CREATE(user_yield, ENV_TYPE_USER);
 #endif // TEST*
-
+    // Starting non-boot CPUs
+	boot_aps();
 	// Schedule and run the first user environment!
 	sched_yield();
 }
@@ -105,6 +98,8 @@ mp_main(void)
 {
 	// We are in high EIP now, safe to switch to kern_pgdir 
 	lcr3(PADDR(kern_pgdir));
+	// now the memory-mapped device content has been
+	// updated with current processor's local apic
 	cprintf("SMP: CPU %d starting\n", cpunum());
 
 	lapic_init();
@@ -117,7 +112,6 @@ mp_main(void)
 	// only one CPU can enter the scheduler at a time!
 	//
 	// Your code here:
-	lock_kernel();
 	sched_yield();
 
 	// Remove this after you finish Exercise 6
diff --git a/kern/lapic.c b/kern/lapic.c
index dc05777..d60aa1e 100644
--- a/kern/lapic.c
+++ b/kern/lapic.c
@@ -21,7 +21,7 @@
 #define ICRLO   (0x0300/4)   // Interrupt Command
 	#define INIT       0x00000500   // INIT/RESET
 	#define STARTUP    0x00000600   // Startup IPI
-	#define DELIVS     0x00001000   // Delivery status
+	#define DELIVS     0x00001000   // Delivery status (read-only, 0: Idle, 1: Send Pending)
 	#define ASSERT     0x00004000   // Assert interrupt (vs deassert)
 	#define DEASSERT   0x00000000
 	#define LEVEL      0x00008000   // Level triggered
@@ -39,7 +39,7 @@
 #define ERROR   (0x0370/4)   // Local Vector Table 3 (ERROR)
 	#define MASKED     0x00010000   // Interrupt masked
 #define TICR    (0x0380/4)   // Timer Initial Count
-#define TCCR    (0x0390/4)   // Timer Current Count
+#define TCCR    (0x0390/4)   // Timer Current Count (read only)
 #define TDCR    (0x03E0/4)   // Timer Divide Configuration
 
 physaddr_t lapicaddr;        // Initialized in mpconfig.c
@@ -63,6 +63,8 @@ lapic_init(void)
 	lapic = mmio_map_region(lapicaddr, 4096);
 
 	// Enable local APIC; set spurious interrupt vector.
+	// The spurious-interrupt vector register is initialized to 000000FFH. 
+	// By setting bit 8 to 0, software disables the local APIC.
 	lapicw(SVR, ENABLE | (IRQ_OFFSET + IRQ_SPURIOUS));
 
 	// The timer repeatedly counts down at bus frequency
@@ -160,12 +162,14 @@ lapic_startap(uint8_t apicid, uint32_t addr)
 	microdelay(200);
 	lapicw(ICRLO, INIT | LEVEL);
 	microdelay(100);    // should be 10ms, but too slow in Bochs!
+	// wait-for-SIPI state
 
 	// Send startup IPI (twice!) to enter code.
 	// Regular hardware is supposed to only accept a STARTUP
 	// when it is in the halted state due to an INIT.  So the second
 	// should be ignored, but it is part of the official Intel algorithm.
 	// Bochs complains about the second one.  Too bad for Bochs.
+	// INIT-SIPI-SIPI sequence
 	for (i = 0; i < 2; i++) {
 		lapicw(ICRHI, apicid << 24);
 		lapicw(ICRLO, STARTUP | (addr >> 12));
diff --git a/kern/pmap.c b/kern/pmap.c
index c2e22a7..a598f78 100644
--- a/kern/pmap.c
+++ b/kern/pmap.c
@@ -10,6 +10,7 @@
 #include <kern/kclock.h>
 #include <kern/env.h>
 #include <kern/cpu.h>
+#include <kern/spinlock.h>
 
 // These variables are set by i386_detect_memory()
 size_t npages;			// Amount of physical memory (in pages)
@@ -387,14 +388,17 @@ struct PageInfo *
 page_alloc(int alloc_flags)
 {
 	// Fill this function in
+	lock_pagelist();
 	if (page_free_list == NULL)
 	{
+		unlock_pagelist();
 		return NULL;
 	}
 
 	struct PageInfo *alloc_page;
 	alloc_page = page_free_list;
 	page_free_list = page_free_list->pp_link;
+	unlock_pagelist();
 	alloc_page->pp_link = NULL;
 
 	if (alloc_flags & ALLOC_ZERO)
@@ -423,8 +427,10 @@ page_free(struct PageInfo *pp)
 	{
 		panic("Page is still in use! Cannot be freed");
 	}
+	lock_pagelist();
 	pp->pp_link = page_free_list;
 	page_free_list = pp;
+	unlock_pagelist();
 }
 
 //
diff --git a/kern/sched.c b/kern/sched.c
index 9e464d7..a9944c5 100644
--- a/kern/sched.c
+++ b/kern/sched.c
@@ -33,6 +33,7 @@ sched_yield(void)
 	int i = 0, curpos = 0, k = 0;
 	if (curenv)
 		curpos = ENVX(curenv->env_id);
+	lock_scheduler();
 	for (; i < NENV; i++)
 	{
 		k = (i + curpos) % NENV;		// in a circular way
@@ -82,7 +83,7 @@ sched_halt(void)
 	xchg(&thiscpu->cpu_status, CPU_HALTED);
 
 	// Release the big kernel lock as if we were "leaving" the kernel
-	unlock_kernel();
+	unlock_scheduler();
 
 	// Reset stack pointer, enable interrupts and then halt.
 	asm volatile (
diff --git a/kern/spinlock.c b/kern/spinlock.c
index bf4d2d2..9a6eea0 100644
--- a/kern/spinlock.c
+++ b/kern/spinlock.c
@@ -16,6 +16,27 @@ struct spinlock kernel_lock = {
 #endif
 };
 
+// the page free list lock
+struct spinlock pagelist_lock = {
+#ifdef DEBUG_SPINLOCK
+	.name = "pagelist_lock"
+#endif
+};
+
+// scheduler lock
+struct spinlock scheduler_lock = {
+#ifdef DEBUG_SPINLOCK
+	.name = "scheduler_lock"
+#endif
+};
+
+// inter-process communication lock
+struct spinlock ipc_lock = {
+#ifdef DEBUG_SPINLOCK
+	.name = "ipc_lock"
+#endif
+};
+
 #ifdef DEBUG_SPINLOCK
 // Record the current call stack in pcs[] by following the %ebp chain.
 static void
diff --git a/kern/spinlock.h b/kern/spinlock.h
index 52d20b4..7aa3a3c 100644
--- a/kern/spinlock.h
+++ b/kern/spinlock.h
@@ -26,6 +26,48 @@ void spin_unlock(struct spinlock *lk);
 #define spin_initlock(lock)   __spin_initlock(lock, #lock)
 
 extern struct spinlock kernel_lock;
+extern struct spinlock pagelist_lock;
+extern struct spinlock scheduler_lock;
+extern struct spinlock ipc_lock;
+
+static inline void
+lock_pagelist(void)
+{
+	spin_lock(&pagelist_lock);
+}
+
+static inline void
+unlock_pagelist(void)
+{
+	spin_unlock(&pagelist_lock);
+	asm volatile("pause");
+}
+
+static inline void
+lock_scheduler(void)
+{
+	spin_lock(&scheduler_lock);
+}
+
+static inline void
+unlock_scheduler(void)
+{
+	spin_unlock(&scheduler_lock);
+	asm volatile("pause");
+}
+
+static inline void
+lock_ipc(void)
+{
+	spin_lock(&ipc_lock);
+}
+
+static inline void
+unlock_ipc(void)
+{
+	spin_unlock(&ipc_lock);
+	asm volatile("pause");
+}
 
 static inline void
 lock_kernel(void)
diff --git a/kern/syscall.c b/kern/syscall.c
index f19a51f..230ecf2 100644
--- a/kern/syscall.c
+++ b/kern/syscall.c
@@ -11,6 +11,7 @@
 #include <kern/syscall.h>
 #include <kern/console.h>
 #include <kern/sched.h>
+#include <kern/spinlock.h>
 
 // Print a string to the system console.
 // The string is exactly 'len' characters long.
@@ -68,6 +69,7 @@ sys_env_destroy(envid_t envid)
 static void
 sys_yield(void)
 {
+	unlock_kernel();
 	sched_yield();
 }
 
@@ -352,6 +354,7 @@ sys_ipc_recv(void *dstva)
 
 	// now we give up CPU
 	curenv->env_status = ENV_NOT_RUNNABLE;
+	unlock_kernel();
 	sched_yield();
 
 	return 0;
diff --git a/kern/trap.c b/kern/trap.c
index 59027a8..a981a75 100644
--- a/kern/trap.c
+++ b/kern/trap.c
@@ -243,6 +243,7 @@ trap_dispatch(struct Trapframe *tf)
 		switch (tf->tf_trapno)
 		{
 		case IRQ_OFFSET + IRQ_TIMER:
+			unlock_kernel();
 			sched_yield();
 			break;
 
@@ -275,19 +276,18 @@ trap(struct Trapframe *tf)
 
 	// Re-acqurie the big kernel lock if we were halted in
 	// sched_yield()
-	if (xchg(&thiscpu->cpu_status, CPU_STARTED) == CPU_HALTED)
-		lock_kernel();
+	xchg(&thiscpu->cpu_status, CPU_STARTED);
+		// lock_scheduler();
 	// Check that interrupts are disabled.  If this assertion
 	// fails, DO NOT be tempted to fix it by inserting a "cli" in
 	// the interrupt path.
 	assert(!(read_eflags() & FL_IF));
+	// Acquire the big kernel lock before doing any
+	// serious kernel work. only apply in trap
+	lock_kernel();
 
 	if ((tf->tf_cs & 3) == 3) {
 		// Trapped from user mode.
-		// Acquire the big kernel lock before doing any
-		// serious kernel work.
-		// LAB 4: Your code here.
-		lock_kernel();
 
 		// Question 2: Why do we still need separate kernel stacks for each CPU?
 		// Answer: 
@@ -302,6 +302,7 @@ trap(struct Trapframe *tf)
 		if (curenv->env_status == ENV_DYING) {
 			env_free(curenv);
 			curenv = NULL;
+			unlock_kernel();
 			sched_yield();
 		}
 
@@ -323,6 +324,7 @@ trap(struct Trapframe *tf)
 	// If we made it to this point, then no other environment was
 	// scheduled, so we should return to the current environment
 	// if doing so makes sense.
+	unlock_kernel();
 	if (curenv && curenv->env_status == ENV_RUNNING)
 		env_run(curenv);
 	else
@@ -408,6 +410,7 @@ page_fault_handler(struct Trapframe *tf)
 
 	tf->tf_esp = (uintptr_t)&utf->utf_fault_va;
 	tf->tf_eip = (uintptr_t)curenv->env_pgfault_upcall;
+	unlock_kernel();
 	env_run(curenv);
 end:
 	// Destroy the environment that caused the fault.
-- 
2.20.1

