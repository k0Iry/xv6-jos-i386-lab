From a4ae2cc7106cda422866519c058c807a4b294046 Mon Sep 17 00:00:00 2001
From: Aaron <kljsandjb@me.com>
Date: Fri, 17 Jul 2020 23:37:08 +0000
Subject: [PATCH] try with fine-grained locks..

adding lock for:
1. page allocator
2. scheduler
3. console
4. ipc state
---
 inc/env.h        |  1 +
 kern/env.c       | 20 ++++++++++---
 kern/init.c      | 12 ++------
 kern/monitor.c   |  6 ----
 kern/pmap.c      |  6 ++++
 kern/sched.c     |  8 +++---
 kern/spinlock.c  | 34 +++++++++++++++++++++--
 kern/spinlock.h  | 85 ++++++++++++++++++++++++++++++++++++++++++++++++++++----
 kern/syscall.c   | 18 +++++++++---
 kern/trap.c      | 12 ++++----
 kern/trapentry.S | 30 ++++++++++----------
 11 files changed, 176 insertions(+), 56 deletions(-)

diff --git a/inc/env.h b/inc/env.h
index b04f1a3..13df7de 100644
--- a/inc/env.h
+++ b/inc/env.h
@@ -32,6 +32,7 @@ typedef int32_t envid_t;
 // Values of env_status in struct Env
 enum {
 	ENV_FREE = 0,
+	ENV_EMBRYO,
 	ENV_DYING,
 	ENV_RUNNABLE,
 	ENV_RUNNING,
diff --git a/kern/env.c b/kern/env.c
index e1019da..fd3da33 100644
--- a/kern/env.c
+++ b/kern/env.c
@@ -220,12 +220,23 @@ env_alloc(struct Env **newenv_store, envid_t parent_id)
 	int r;
 	struct Env *e;
 
+	lock_env();
 	if (!(e = env_free_list))
+	{
+		unlock_env();
 		return -E_NO_FREE_ENV;
+	}
 
 	// Allocate and set up the page directory for this environment.
 	if ((r = env_setup_vm(e)) < 0)
+	{
+		unlock_env();
 		return r;
+	}
+
+	// commit the allocation
+	env_free_list = e->env_link;
+	unlock_env();
 
 	// Generate an env_id for this environment.
 	generation = (e->env_id + (1 << ENVGENSHIFT)) & ~(NENV - 1);
@@ -236,7 +247,7 @@ env_alloc(struct Env **newenv_store, envid_t parent_id)
 	// Set the basic status variables.
 	e->env_parent_id = parent_id;
 	e->env_type = ENV_TYPE_USER;
-	e->env_status = ENV_RUNNABLE;
+	e->env_status = ENV_EMBRYO;
 	e->env_runs = 0;
 
 	// Clear out all the saved register state,
@@ -270,8 +281,6 @@ env_alloc(struct Env **newenv_store, envid_t parent_id)
 	// Also clear the IPC receiving flag.
 	e->env_ipc_recving = 0;
 
-	// commit the allocation
-	env_free_list = e->env_link;
 	*newenv_store = e;
 
 	cprintf("[%08x] new env %08x\n", curenv ? curenv->env_id : 0, e->env_id);
@@ -417,6 +426,7 @@ env_create(uint8_t *binary, enum EnvType type)
 		panic("env_alloc");
 	load_icode(new_env, binary);
 	new_env->env_type = type;
+	new_env->env_status = ENV_RUNNABLE;
 }
 
 //
@@ -468,8 +478,10 @@ env_free(struct Env *e)
 
 	// return the environment to the free list
 	e->env_status = ENV_FREE;
+	lock_env();
 	e->env_link = env_free_list;
 	env_free_list = e;
+	unlock_env();
 }
 
 //
@@ -557,7 +569,7 @@ env_run(struct Env *e)
 	curenv->env_runs ++;
 
 	lcr3(PADDR(curenv->env_pgdir));
-	unlock_kernel();
+	unlock_sched();
 	env_pop_tf(&(curenv->env_tf));
 }
 
diff --git a/kern/init.c b/kern/init.c
index 0a0faaa..0b77d26 100644
--- a/kern/init.c
+++ b/kern/init.c
@@ -41,13 +41,6 @@ i386_init(void)
 	// Lab 4 multitasking initialization functions
 	pic_init();
 
-	// Acquire the big kernel lock before waking up APs
-	// Your code here:
-	lock_kernel();
-
-	// Starting non-boot CPUs
-	boot_aps();
-
 #if defined(TEST)
 	// Don't touch -- used by grading script!
 	ENV_CREATE(TEST, ENV_TYPE_USER);
@@ -62,8 +55,8 @@ i386_init(void)
 	ENV_CREATE(user_yield, ENV_TYPE_USER);
 	ENV_CREATE(user_yield, ENV_TYPE_USER);
 #endif // TEST*
-
-	// Schedule and run the first user environment!
+	// tasks are all ready, now boot application cpus and start scheduling
+	boot_aps();
 	sched_yield();
 }
 
@@ -117,7 +110,6 @@ mp_main(void)
 	// only one CPU can enter the scheduler at a time!
 	//
 	// Your code here:
-	lock_kernel();
 	sched_yield();
 
 	// Remove this after you finish Exercise 6
diff --git a/kern/monitor.c b/kern/monitor.c
index c46e3fb..ac6a91e 100644
--- a/kern/monitor.c
+++ b/kern/monitor.c
@@ -145,20 +145,14 @@ int mon_debug(int argc, char **argv, struct Trapframe *tf)
 	}
 
 	extern void env_pop_tf(struct Trapframe *tf);
-	extern struct spinlock kernel_lock;
-	void spin_unlock(struct spinlock *lk);
 	if (strncmp("si", argv[1], 10) == 0)
 	{
-		spin_unlock(&kernel_lock);
-		asm volatile("pause");
 		env_pop_tf(tf);
 	}
 	if (strncmp("continue", argv[1], 10) == 0)
 	{
 		// clear TF flag to disable IRQ1
 		tf->tf_eflags &= ~0x100;
-		spin_unlock(&kernel_lock);
-		asm volatile("pause");
 		env_pop_tf(tf);
 	}
 
diff --git a/kern/pmap.c b/kern/pmap.c
index c2e22a7..5d4eae3 100644
--- a/kern/pmap.c
+++ b/kern/pmap.c
@@ -10,6 +10,7 @@
 #include <kern/kclock.h>
 #include <kern/env.h>
 #include <kern/cpu.h>
+#include <kern/spinlock.h>
 
 // These variables are set by i386_detect_memory()
 size_t npages;			// Amount of physical memory (in pages)
@@ -387,14 +388,17 @@ struct PageInfo *
 page_alloc(int alloc_flags)
 {
 	// Fill this function in
+	lock_page();
 	if (page_free_list == NULL)
 	{
+		unlock_page();
 		return NULL;
 	}
 
 	struct PageInfo *alloc_page;
 	alloc_page = page_free_list;
 	page_free_list = page_free_list->pp_link;
+	unlock_page();
 	alloc_page->pp_link = NULL;
 
 	if (alloc_flags & ALLOC_ZERO)
@@ -423,8 +427,10 @@ page_free(struct PageInfo *pp)
 	{
 		panic("Page is still in use! Cannot be freed");
 	}
+	lock_page();
 	pp->pp_link = page_free_list;
 	page_free_list = pp;
+	unlock_page();
 }
 
 //
diff --git a/kern/sched.c b/kern/sched.c
index 9e464d7..97b7aeb 100644
--- a/kern/sched.c
+++ b/kern/sched.c
@@ -11,8 +11,6 @@ void sched_halt(void);
 void
 sched_yield(void)
 {
-	struct Env *idle;
-
 	// Implement simple round-robin scheduling.
 	// https://www.geeksforgeeks.org/program-round-robin-scheduling-set-1/
 	//
@@ -33,6 +31,7 @@ sched_yield(void)
 	int i = 0, curpos = 0, k = 0;
 	if (curenv)
 		curpos = ENVX(curenv->env_id);
+	lock_sched();
 	for (; i < NENV; i++)
 	{
 		k = (i + curpos) % NENV;		// in a circular way
@@ -67,6 +66,7 @@ sched_halt(void)
 			break;
 	}
 	if (i == NENV) {
+		// unlock_sched();
 		cprintf("No runnable environments in the system!\n");
 		while (1)
 			monitor(NULL);
@@ -82,7 +82,7 @@ sched_halt(void)
 	xchg(&thiscpu->cpu_status, CPU_HALTED);
 
 	// Release the big kernel lock as if we were "leaving" the kernel
-	unlock_kernel();
+	unlock_sched();
 
 	// Reset stack pointer, enable interrupts and then halt.
 	asm volatile (
@@ -94,7 +94,7 @@ sched_halt(void)
 		"sti\n"
 		"1:\n"
 		"hlt\n"
-		"jmp 1b\n"
+		// "jmp 1b\n"
 	: : "a" (thiscpu->cpu_ts.ts_esp0));
 }
 
diff --git a/kern/spinlock.c b/kern/spinlock.c
index bf4d2d2..a350ba1 100644
--- a/kern/spinlock.c
+++ b/kern/spinlock.c
@@ -9,10 +9,38 @@
 #include <kern/spinlock.h>
 #include <kern/kdebug.h>
 
-// The big kernel lock
-struct spinlock kernel_lock = {
+// memory allocation lock (alloc, free)
+struct spinlock page_lock = {
 #ifdef DEBUG_SPINLOCK
-	.name = "kernel_lock"
+	.name = "page_lock"
+#endif
+};
+
+// scheduler lock (picking tasks)
+struct spinlock sched_lock = {
+#ifdef DEBUG_SPINLOCK
+	.name = "sched_lock"
+#endif
+};
+
+// scheduler lock
+struct spinlock cons_lock = {
+#ifdef DEBUG_SPINLOCK
+	.name = "cons_lock"
+#endif
+};
+
+// env list lock
+struct spinlock env_lock = {
+#ifdef DEBUG_SPINLOCK
+	.name = "env_lock"
+#endif
+};
+
+// ipc state lock
+struct spinlock ipc_lock = {
+#ifdef DEBUG_SPINLOCK
+	.name = "ipc_lock"
 #endif
 };
 
diff --git a/kern/spinlock.h b/kern/spinlock.h
index 52d20b4..11c67ba 100644
--- a/kern/spinlock.h
+++ b/kern/spinlock.h
@@ -25,18 +25,22 @@ void spin_unlock(struct spinlock *lk);
 
 #define spin_initlock(lock)   __spin_initlock(lock, #lock)
 
-extern struct spinlock kernel_lock;
+extern struct spinlock page_lock;
+extern struct spinlock sched_lock;
+extern struct spinlock cons_lock;
+extern struct spinlock env_lock;
+extern struct spinlock ipc_lock;
 
 static inline void
-lock_kernel(void)
+lock_page(void)
 {
-	spin_lock(&kernel_lock);
+	spin_lock(&page_lock);
 }
 
 static inline void
-unlock_kernel(void)
+unlock_page(void)
 {
-	spin_unlock(&kernel_lock);
+	spin_unlock(&page_lock);
 
 	// Normally we wouldn't need to do this, but QEMU only runs
 	// one CPU at a time and has a long time-slice.  Without the
@@ -45,4 +49,75 @@ unlock_kernel(void)
 	asm volatile("pause");
 }
 
+static inline void
+lock_sched(void)
+{
+	spin_lock(&sched_lock);
+}
+
+static inline void
+unlock_sched(void)
+{
+	spin_unlock(&sched_lock);
+
+	// Normally we wouldn't need to do this, but QEMU only runs
+	// one CPU at a time and has a long time-slice.  Without the
+	// pause, this CPU is likely to reacquire the lock before
+	// another CPU has even been given a chance to acquire it.
+	asm volatile("pause");
+}
+
+static inline void
+lock_cons(void)
+{
+	spin_lock(&cons_lock);
+}
+
+static inline void
+unlock_cons(void)
+{
+	spin_unlock(&cons_lock);
+
+	// Normally we wouldn't need to do this, but QEMU only runs
+	// one CPU at a time and has a long time-slice.  Without the
+	// pause, this CPU is likely to reacquire the lock before
+	// another CPU has even been given a chance to acquire it.
+	asm volatile("pause");
+}
+
+static inline void
+lock_env(void)
+{
+	spin_lock(&env_lock);
+}
+
+static inline void
+unlock_env(void)
+{
+	spin_unlock(&env_lock);
+
+	// Normally we wouldn't need to do this, but QEMU only runs
+	// one CPU at a time and has a long time-slice.  Without the
+	// pause, this CPU is likely to reacquire the lock before
+	// another CPU has even been given a chance to acquire it.
+	asm volatile("pause");
+}
+
+static inline void
+lock_ipc(void)
+{
+	spin_lock(&ipc_lock);
+}
+
+static inline void
+unlock_ipc(void)
+{
+	spin_unlock(&ipc_lock);
+
+	// Normally we wouldn't need to do this, but QEMU only runs
+	// one CPU at a time and has a long time-slice.  Without the
+	// pause, this CPU is likely to reacquire the lock before
+	// another CPU has even been given a chance to acquire it.
+	asm volatile("pause");
+}
 #endif
diff --git a/kern/syscall.c b/kern/syscall.c
index f19a51f..642caec 100644
--- a/kern/syscall.c
+++ b/kern/syscall.c
@@ -11,6 +11,7 @@
 #include <kern/syscall.h>
 #include <kern/console.h>
 #include <kern/sched.h>
+#include <kern/spinlock.h>
 
 // Print a string to the system console.
 // The string is exactly 'len' characters long.
@@ -25,7 +26,9 @@ sys_cputs(const char *s, size_t len)
 	user_mem_assert(curenv, s, len, PTE_U);
 
 	// Print the string supplied by the user.
+	lock_cons();
 	cprintf("%.*s", len, s);
+	unlock_cons();
 }
 
 // Read a character from the system console without blocking.
@@ -302,8 +305,6 @@ sys_ipc_try_send(envid_t envid, uint32_t value, void *srcva, unsigned perm)
 	pte_t *pgtbl_entry = NULL;
 	if ((r = envid2env(envid, &env, 0)) != 0)
 		return r;
-	if (env->env_ipc_recving == 0)
-		return -E_IPC_NOT_RECV;
 	if ((uintptr_t)srcva < UTOP && (uintptr_t)srcva % PGSIZE)
 		return -E_INVAL;
 	if ((uintptr_t)srcva < UTOP && ((perm & PTE_P) != PTE_P || (perm & PTE_U) != PTE_U || (perm & ~PTE_SYSCALL) != 0))
@@ -313,6 +314,14 @@ sys_ipc_try_send(envid_t envid, uint32_t value, void *srcva, unsigned perm)
 	if ((uintptr_t)srcva < UTOP && (perm & PTE_W) && !(*pgtbl_entry & PTE_W))
 		return -E_INVAL;
 
+	lock_ipc();
+	if (env->env_ipc_recving == 0)
+	{
+		unlock_ipc();
+		return -E_IPC_NOT_RECV;
+	}
+	env->env_ipc_recving = 0;
+	unlock_ipc();
 	env->env_ipc_perm = 0;
 	if ((uintptr_t)srcva < UTOP && (uintptr_t)(env->env_ipc_dstva) < UTOP)
 	{
@@ -321,11 +330,10 @@ sys_ipc_try_send(envid_t envid, uint32_t value, void *srcva, unsigned perm)
 			return r;
 		env->env_ipc_perm = perm;	// update perm if there is actually a page being transferred
 	}
-	env->env_ipc_recving = 0;
 	env->env_ipc_from = curenv->env_id;
 	env->env_ipc_value = value;
-	env->env_status = ENV_RUNNABLE;
 	env->env_tf.tf_regs.reg_eax = 0;	// next time receiver being woken-up, it returns 0 and got the value or page it needs
+	env->env_status = ENV_RUNNABLE;
 
 	return 0;
 }
@@ -347,7 +355,9 @@ sys_ipc_recv(void *dstva)
 	// LAB 4: Your code here.
 	if ((uintptr_t)dstva < UTOP && (uintptr_t)dstva % PGSIZE)
 		return -E_INVAL;
+	lock_ipc();
 	curenv->env_ipc_recving = 1;	// ready for receiving something
+	unlock_ipc();
 	curenv->env_ipc_dstva = dstva;	// tell sender if we want a page
 
 	// now we give up CPU
diff --git a/kern/trap.c b/kern/trap.c
index 59027a8..6e14d06 100644
--- a/kern/trap.c
+++ b/kern/trap.c
@@ -273,10 +273,9 @@ trap(struct Trapframe *tf)
 	if (panicstr)
 		asm volatile("hlt");
 
-	// Re-acqurie the big kernel lock if we were halted in
-	// sched_yield()
-	if (xchg(&thiscpu->cpu_status, CPU_STARTED) == CPU_HALTED)
-		lock_kernel();
+	// wake up halted cpu
+	if (thiscpu->cpu_status == CPU_HALTED)
+		xchg(&thiscpu->cpu_status, CPU_STARTED);
 	// Check that interrupts are disabled.  If this assertion
 	// fails, DO NOT be tempted to fix it by inserting a "cli" in
 	// the interrupt path.
@@ -287,7 +286,6 @@ trap(struct Trapframe *tf)
 		// Acquire the big kernel lock before doing any
 		// serious kernel work.
 		// LAB 4: Your code here.
-		lock_kernel();
 
 		// Question 2: Why do we still need separate kernel stacks for each CPU?
 		// Answer: 
@@ -324,7 +322,10 @@ trap(struct Trapframe *tf)
 	// scheduled, so we should return to the current environment
 	// if doing so makes sense.
 	if (curenv && curenv->env_status == ENV_RUNNING)
+	{
+		lock_sched();
 		env_run(curenv);
+	}
 	else
 		sched_yield();
 }
@@ -408,6 +409,7 @@ page_fault_handler(struct Trapframe *tf)
 
 	tf->tf_esp = (uintptr_t)&utf->utf_fault_va;
 	tf->tf_eip = (uintptr_t)curenv->env_pgfault_upcall;
+	lock_sched();
 	env_run(curenv);
 end:
 	// Destroy the environment that caused the fault.
diff --git a/kern/trapentry.S b/kern/trapentry.S
index aa6bd00..039cc94 100644
--- a/kern/trapentry.S
+++ b/kern/trapentry.S
@@ -60,15 +60,15 @@ default_handlers:
 # (about popf) The I/O privilege level is altered only when executing at privilege level 0. 
 # The interrupt flag is altered only when executing at a level at least as privileged as the I/O privilege level.
 sysenter_handler:
-pushl %ecx
-pushl %edx
-pushl %eax
-pushl $kernel_lock
-call spin_lock
-addl $4, %esp
-popl %eax
-popl %edx
-popl %ecx
+# pushl %ecx
+# pushl %edx
+# pushl %eax
+# pushl $kernel_lock
+# call spin_lock
+# addl $4, %esp
+# popl %eax
+# popl %edx
+# popl %ecx
 
 pushl %edi
 pushl %ebx
@@ -81,12 +81,12 @@ call syscall
 # movl $0, %edx
 # movl $(GD_UT), %eax	/* no need, because of continuity, GD_UT will be found by adding 16(0x10) to GD_KT */
 # wrmsr
-pushl %eax
-pushl $kernel_lock
-call spin_unlock
-pause
-addl $4, %esp
-popl %eax
+# pushl %eax
+# pushl $kernel_lock
+# call spin_unlock
+# pause
+# addl $4, %esp
+# popl %eax
 
 movl %esi, %edx
 movl %ebp, %ecx
-- 
2.11.0

